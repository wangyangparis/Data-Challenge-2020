{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKgYaz3BGakb"
   },
   "source": [
    "# Challenge Large Scale Machine Learning\n",
    "## Fusion of algorithms for face recognition\n",
    "Authors:\n",
    "Pavlo Mozharovskyi (pavlo.mozharovskyi@telecom-paris.fr), Umut Şimşekli (umut.simsekli@telecom-paris.fr)\n",
    "\n",
    "\n",
    "\n",
    "The increasingly ubiquitous presence of biometric solutions and face recognition in particular in everyday life requires their adaptation for practical scenario. In the presence of several possible solutions, and if global decisions are to be made, each such single solution can be far less efficient than tailoring them to the complexity of an image.\n",
    "\n",
    "In this challenge, the goal is to build a fusion of algorithms in order to construct the best suited solution for comparison of a pair of images. This fusion will be driven by qualities computed on each image.\n",
    "\n",
    "Comparing of two images is done in two steps. 1st, a vector of features is computed for each image. 2nd, a simple function produces a vector of scores for a pair of images. The goal is to create a function that will compare a pair of images based on the information mentioned above, and decide whether two images belong to the same person.\n",
    "\n",
    "You are provided a label set of training data and a test set without labels. You should submit a .csv file with labels for each entry of this test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fFOHb8lG0MV"
   },
   "source": [
    "<h2 id=\"The-properties-of-the-dataset:\">The properties of the dataset:<a class=\"anchor-link\" href=\"#The-properties-of-the-dataset:\">&#182;</a></h2>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<h3 id=\"Training-data:\">Training data:<a class=\"anchor-link\" href=\"#Training-data:\">&#182;</a></h3>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<p>The training set consist of two files, <strong>xtrain_challenge.csv</strong> and <strong>xtest_challenge.csv</strong>.</p>\n",
    "<p>File <strong>xtrain_challenge.csv</strong> contains one observation per row which contains following entries based on a pair of images:</p>\n",
    "<ul>\n",
    "<li>columns 1-13 - 13 qualities on first image;</li>\n",
    "<li>columns 14-26 - 13 qualities on second image;</li>\n",
    "<li>columns 27-37 - 11 matching scores between the two images.</li>\n",
    "</ul>\n",
    "<p>File <strong>ytrain_challenge.csv</strong> contains one line with each entry corresponding to one observation in <strong>xtrain_challenge.csv</strong>, maintaining the order, and has '1' if a pair of images belong to the same person and '0' otherwise.</p>\n",
    "<p>For each of these 38 variables, there are in total 9,800,713 training observations.</p>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div >\n",
    "<h3 id=\"Test-data:\">Test data:<a class=\"anchor-link\" href=\"#Test-data:\">&#182;</a></h3>\n",
    "</div>\n",
    "\n",
    "<p>File <strong>xtest_challenge.csv</strong> has the same structure as file <strong>xtrain_challenge.csv</strong>.</p>\n",
    "<p>There are in total 3,768,311 test observations.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"The-performance-criterion&#182;\">The performance criterion&#182;</h2>\n",
    "Consider the problem of the supervised classification with two classes labeled '0' and '1'. Many methods for supervised classification assign a new observation $\\boldsymbol{x}$ to a class using the following rule:</p>\n",
    "\\begin{align}\n",
    "g(\\boldsymbol{x}) = \\begin{cases}1 &amp; \\text{ if } f(\\boldsymbol{x}) \\ge t, \\\\ 0 &amp; \\text{ otherwise}.\\end{cases}\n",
    "\\end{align}<p>Threshold $t$ is then chosen due to specific needs managing the trade-off between the true positive rate (TPR) and the false positive rate (FPR), depending on the cost of the corresponding mistakes.</p>\n",
    "<p>Here, the performance criterion is <strong>TPR for the value of FPR = $10^{-4}$</strong>, or, speaking in other words, one needs to maximize the value of the receiver operating characteristic (ROC) in the point FPR = $10^{-4}$. The submitted solution file should thus contain the score for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports \n",
    "### Pythons packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:01:09.439162Z",
     "start_time": "2020-03-11T15:01:06.701439Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eKRhjXwl7VQ0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "### General imports ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "### Visualization ###\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "#from ggplot import *\n",
    "from matplotlib.collections import EllipseCollection\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "###metrics###\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "### Build the model ###\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "### Machine Learning ###\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount google drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5-9K8UcDHbBu",
    "outputId": "20c6817c-127b-47f0-916b-6156cf37a2ae"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T13:43:24.953422Z",
     "start_time": "2020-02-16T13:43:24.929411Z"
    }
   },
   "source": [
    "### import datasets\n",
    "#### on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T17:30:30.740077Z",
     "start_time": "2020-03-03T17:30:30.237342Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "di248TVA7cSS",
    "outputId": "e5898ba0-a0fb-4207-d5ba-238558d7c9f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "nrows_train = 9000000\n",
    "nrows_valid = 800713\n",
    "xtrain = np.loadtxt('/content/drive/My Drive/DataChallenge2020MSBGD/xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "ytrain = np.loadtxt('/content/drive/My Drive/DataChallenge2020MSBGD/ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "ytrain = np.array(ytrain).reshape(nrows_train + nrows_valid)\n",
    "\n",
    "ytrain = np.array(ytrain).reshape(nrows_train + nrows_valid)\n",
    "\n",
    "x_train=xtrain[:nrows_train]\n",
    "y_train=ytrain[:nrows_train]\n",
    "x_test=xtrain[nrows_train:(nrows_train + nrows_valid)] \n",
    "y_test=ytrain[nrows_train:(nrows_train + nrows_valid)] \n",
    "\n",
    "# Check the number of observations and properties\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on local Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:18:53.843060Z",
     "start_time": "2020-03-11T15:18:48.655743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "nrows_train = 8000\n",
    "nrows_valid = 100000 #800713\n",
    "xtrain = np.loadtxt('xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "ytrain = np.loadtxt('ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "\n",
    "ytrain = np.array(ytrain).reshape(nrows_train + nrows_valid)\n",
    "\n",
    "x_train=xtrain[:nrows_train]\n",
    "y_train=ytrain[:nrows_train]\n",
    "x_test=xtrain[nrows_train:(nrows_train + nrows_valid)] \n",
    "y_test=ytrain[nrows_train:(nrows_train + nrows_valid)] \n",
    "\n",
    "# Check the number of observations and properties\n",
    "print(xtrain[:1,0:12]) #1-13  #13\n",
    "print(xtrain[:1,13:25]) #14-26  #13\n",
    "print(xtrain[:1,26:36]) #27-37   #11\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:48:51.897341Z",
     "start_time": "2020-02-13T17:48:51.894849Z"
    }
   },
   "source": [
    "# Exploratory data analysis\n",
    "## Duplicate and missing values detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:31:15.466908Z",
     "start_time": "2020-03-11T20:31:15.380213Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train[:800], columns=['fA1', 'fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA8', 'fA9', 'fA10', 'fA11',\n",
    "                                            'fA12', 'fA13', 'fB1', 'fB2', 'fB3', 'fB4', 'fB5', 'fB6', 'fB7', 'fB8', 'fB9', 'fB10', 'fB11', 'fB12', 'fB13',\n",
    "                                            'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11'])\n",
    "y_train_df = pd.DataFrame(y_train[:800])\n",
    "\n",
    "x_test_df = pd.DataFrame(x_test[:800], columns=['fA1', 'fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA8', 'fA9', 'fA10', 'fA11',\n",
    "                                          'fA12', 'fA13', 'fB1', 'fB2', 'fB3', 'fB4', 'fB5', 'fB6', 'fB7', 'fB8', 'fB9', 'fB10', 'fB11', 'fB12', 'fB13',\n",
    "                                          'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T19:45:00.154463Z",
     "start_time": "2020-02-24T19:44:59.701838Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array(x_train[:, 4])\n",
    "np.unique(a, axis=0)\n",
    "u, indices, counts = np.unique(a, return_index=True, return_counts=True)\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T19:45:01.744944Z",
     "start_time": "2020-02-24T19:45:01.154755Z"
    }
   },
   "outputs": [],
   "source": [
    "uc = x_train_df.nunique()\n",
    "uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:37:26.455400Z",
     "start_time": "2020-03-11T20:36:51.180199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Parcoords(\n",
    "    line=dict(color=x_train[:, 0],\n",
    "              colorscale='Electric',\n",
    "              showscale=True,\n",
    "              ),\n",
    "    dimensions=list([\n",
    "        dict(range=[x_train[:, 0].min(), x_train[:, 0].max()],\n",
    "             label=\"fA1\", values=x_train[:, 0]),\n",
    "        dict(range=[x_train[:, 1].min(), x_train[:, 1].max()],\n",
    "             label='fA2', values=x_train[:, 1]),\n",
    "        dict(range=[x_train[:, 2].min(), x_train[:, 2].max()],\n",
    "             tickvals=[0, 0.5, 1, 2, 3],\n",
    "             ticktext=['A', 'AB', 'B', 'Y', 'Z'],\n",
    "             label='fA3', values=x_train[:, 2]),\n",
    "        dict(range=[x_train[:, 3].min(), x_train[:, 3].max()],\n",
    "             tickvals=[0, 1, 2, 3],\n",
    "             label='fA4', values=x_train[:, 3]),\n",
    "        dict(range=[x_train[:, 4].min(), x_train[:, 4].max()],\n",
    "             visible=True,\n",
    "             label='fA5', values=x_train[:, 4]),\n",
    "        dict(range=[x_train[:, 5].min(), x_train[:, 5].max()],\n",
    "             label='fA6', values=x_train[:, 5]),\n",
    "        dict(range=[x_train[:, 6].min(), x_train[:, 6].max()],\n",
    "             label='fA7', values=x_train[:, 6]),\n",
    "        dict(range=[x_train[:, 7].min(), x_train[:, 7].max()],\n",
    "             label='fA 8 ', values=x_train[:, 7]),\n",
    "        dict(range=[x_train[:, 8].min(), x_train[:, 8].max()],\n",
    "             label=\"fA 9 \", values=x_train[:, 8]),\n",
    "        dict(range=[x_train[:, 9].min(), x_train[:, 9].max()],\n",
    "             label=\"fA 10 \", values=x_train[:, 9]),\n",
    "        dict(range=[x_train[:, 10].min(), x_train[:, 10].max()],\n",
    "             label=\"fA 11 \", values=x_train[:, 10]),\n",
    "        dict(range=[x_train[:, 11].min(), x_train[:, 11].max()],\n",
    "             label=\"fA 12 \", values=x_train[:, 11]),\n",
    "        dict(range=[x_train[:, 12].min(), x_train[:, 12].max()],\n",
    "             label=\"fA 13 \", values=x_train[:, 12]),\n",
    "    ]),\n",
    ")\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=go.Parcoords(\n",
    "    line=dict(color=x_train[:, 13],  # colorscale='Electric',\n",
    "              showscale=True,\n",
    "              ),\n",
    "    dimensions=list([\n",
    "        dict(range=[x_train[:, 13].min(), x_train[:, 13].max()],\n",
    "             label=\"fB 1 \", values=x_train[:, 13]),\n",
    "        dict(range=[x_train[:, 14].min(), x_train[:, 14].max()],\n",
    "             label=\"fB 2 \", values=x_train[:, 14]),\n",
    "        dict(range=[x_train[:, 15].min(), x_train[:, 15].max()],\n",
    "             label=\"fB 3 \", values=x_train[:, 15]),\n",
    "        dict(range=[x_train[:, 16].min(), x_train[:, 16].max()],\n",
    "             label=\"fB 4 \", values=x_train[:, 16]),\n",
    "        dict(range=[x_train[:, 17].min(), x_train[:, 17].max()],\n",
    "             label=\"fB 5 \", values=x_train[:, 17]),\n",
    "        dict(range=[x_train[:, 18].min(), x_train[:, 18].max()],\n",
    "             label=\"fB 6 \", values=x_train[:, 18]),\n",
    "        dict(range=[x_train[:, 19].min(), x_train[:, 19].max()],\n",
    "             label=\"fB 7 \", values=x_train[:, 19]),\n",
    "        dict(range=[x_train[:, 20].min(), x_train[:, 20].max()],\n",
    "             label=\"fB 8 \", values=x_train[:, 20]),\n",
    "        dict(range=[x_train[:, 21].min(), x_train[:, 21].max()],\n",
    "             label=\"fB 9 \", values=x_train[:, 21]),\n",
    "        dict(range=[x_train[:, 22].min(), x_train[:, 22].max()],\n",
    "             label=\"fB 10 \", values=x_train[:, 22]),\n",
    "        dict(range=[x_train[:, 23].min(), x_train[:, 23].max()],\n",
    "             label=\"fB 11 \", values=x_train[:, 23]),\n",
    "        dict(range=[x_train[:, 24].min(), x_train[:, 24].max()],\n",
    "             label=\"fB 12 \", values=x_train[:, 24]),\n",
    "        dict(range=[x_train[:, 25].min(), x_train[:, 25].max()],\n",
    "             label=\"fB 13 \", values=x_train[:, 25]),\n",
    "    ]),\n",
    ")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:37:47.389017Z",
     "start_time": "2020-03-11T20:37:26.460962Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Parcoords(\n",
    "    line=dict(  color = y_train,#colorscale='Electric',\n",
    "        showscale=True,\n",
    "             ) ,\n",
    "    dimensions=list([\n",
    "        dict(range=[(x_train[:, 0]-x_train[:, 13]).min(), (x_train[:, 0]-x_train[:,\n",
    "                                                                                 13]).max()], label=\"fA-fB 0 \", values=x_train[:, 0]-x_train[:, 13]),\n",
    "        dict(range=[(x_train[:, 1]-x_train[:, 14]).min(), (x_train[:, 1]-x_train[:,\n",
    "                                                                                 14]).max()], label=\"fA-fB 1 \", values=x_train[:, 1]-x_train[:, 14]),\n",
    "        dict(range=[(x_train[:, 2]-x_train[:, 15]).min(), (x_train[:, 2]-x_train[:,\n",
    "                                                                                 15]).max()], label=\"fA-fB 2 \", values=x_train[:, 2]-x_train[:, 15]),\n",
    "        dict(range=[(x_train[:, 3]-x_train[:, 16]).min(), (x_train[:, 3]-x_train[:,\n",
    "                                                                                 16]).max()], label=\"fA-fB 3 \", values=x_train[:, 3]-x_train[:, 16]),\n",
    "        dict(range=[(x_train[:, 4]-x_train[:, 17]).min(), (x_train[:, 4]-x_train[:,\n",
    "                                                                                 17]).max()], label=\"fA-fB 4 \", values=x_train[:, 4]-x_train[:, 17]),\n",
    "        dict(range=[(x_train[:, 5]-x_train[:, 18]).min(), (x_train[:, 5]-x_train[:,\n",
    "                                                                                 18]).max()], label=\"fA-fB 5 \", values=x_train[:, 5]-x_train[:, 18]),\n",
    "        dict(range=[(x_train[:, 6]-x_train[:, 19]).min(), (x_train[:, 6]-x_train[:,\n",
    "                                                                                 19]).max()], label=\"fA-fB 6 \", values=x_train[:, 6]-x_train[:, 19]),\n",
    "        dict(range=[(x_train[:, 7]-x_train[:, 20]).min(), (x_train[:, 7]-x_train[:,\n",
    "                                                                                 20]).max()], label=\"fA-fB 7 \", values=x_train[:, 7]-x_train[:, 20]),\n",
    "        dict(range=[(x_train[:, 8]-x_train[:, 21]).min(), (x_train[:, 8]-x_train[:,\n",
    "                                                                                 21]).max()], label=\"fA-fB 8 \", values=x_train[:, 8]-x_train[:, 21]),\n",
    "        dict(range=[(x_train[:, 9]-x_train[:, 22]).min(), (x_train[:, 9]-x_train[:,\n",
    "                                                                                 22]).max()], label=\"fA-fB 9 \", values=x_train[:, 9]-x_train[:, 22]),\n",
    "        dict(range=[(x_train[:, 10]-x_train[:, 23]).min(), (x_train[:, 10]-x_train[:,\n",
    "                                                                                   23]).max()], label=\"fA-fB 10 \", values=x_train[:, 10]-x_train[:, 23]),\n",
    "        dict(range=[(x_train[:, 11]-x_train[:, 24]).min(), (x_train[:, 11]-x_train[:,\n",
    "                                                                                   24]).max()], label=\"fA-fB 11 \", values=x_train[:, 11]-x_train[:, 24]),\n",
    "        dict(range=[(x_train[:, 12]-x_train[:, 25]).min(), (x_train[:, 12]-x_train[:,\n",
    "                                                                                   25]).max()], label=\"fA-fB 12 \", values=x_train[:, 12]-x_train[:, 25]),\n",
    "    \n",
    "        dict(range=[-0.1,1], label=\"y\", values=y_train),\n",
    "    \n",
    "    \n",
    "    ]),\n",
    ")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T21:53:47.803044Z",
     "start_time": "2020-02-24T21:53:46.963823Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(range(len(uc)), uc, label='number of unique values',)\n",
    "#plt.bar(range(len(clf.feature_importances_)),clf.feature_importances_,label='model with new features',alpha=0.6)\n",
    "plt.xticks(range(len(uc)), ('fA1', 'fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA8', 'fA9', 'fA10', 'fA11',\n",
    "                            'fA12', 'fA13', 'fB1', 'fB2', 'fB3', 'fB4', 'fB5', 'fB6', 'fB7', 'fB8', 'fB9', 'fB10', 'fB11', 'fB12', 'fB13',\n",
    "                                            'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11'))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T21:54:00.170172Z",
     "start_time": "2020-02-24T21:53:59.906283Z"
    }
   },
   "outputs": [],
   "source": [
    "p1_unique = x_train_df.iloc[:, :12].drop_duplicates()\n",
    "p2_unique = x_train_df.iloc[:, 13:25].drop_duplicates()\n",
    "p1_n = len(p1_unique)\n",
    "p2_n = len(p2_unique)\n",
    "n = len(ytrain)\n",
    "\n",
    "print(\"The n of unique picture 1: \", p1_n, \"duplication : \", 1 - p1_n/n)\n",
    "print(\"The n of unique picture 2: \", p2_n, \"duplication : \", 1 - p2_n/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of each variable/feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:24:49.395225Z",
     "start_time": "2020-03-11T20:15:52.426197Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(15,15))\n",
    "x_df=pd.DataFrame(x_train[:100,])\n",
    "sns.pairplot(x_df,  palette = \"colorblind\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T21:55:11.404173Z",
     "start_time": "2020-02-24T21:54:20.893007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "EhJHx4-wxAvC",
    "outputId": "4e821c73-1939-445f-a47a-cffc3edf3041"
   },
   "outputs": [],
   "source": [
    "# we plot to see the correlation between each feature and the labels\n",
    "sns.set()\n",
    "plt.figure(figsize=(16, 30))\n",
    "j = 0\n",
    "for i in range(0, 37, 1):\n",
    "    j = j+1\n",
    "    plt.subplot(13, 3, j)\n",
    "    plt.plot(x_train[:, i], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T22:06:07.276733Z",
     "start_time": "2020-02-24T22:01:41.267464Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "whbKNEzWW0EE",
    "outputId": "7dfba418-bfe1-49ad-8db4-0b814aa05e72"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(16, 30))\n",
    "j = 0\n",
    "for i in range(0, 37, 1):\n",
    "    j = j+1\n",
    "    plt.subplot(13, 3, j)\n",
    "    plt.scatter(xtrain[:, i], ytrain, alpha=0.05)\n",
    "    plt.scatter(xtrain[:, i][ytrain == 1].mean(), 1, label='mean of class 1')\n",
    "    plt.scatter(xtrain[:, i][ytrain == 0].mean(), 0, label='mean of class 0')\n",
    "    plt.title('feature '+str(i))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T21:54:18.593281Z",
     "start_time": "2020-02-24T21:54:03.950513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "ytrain[xtrain[:, i] == 1]\n",
    "plt.figure(figsize=(16, 22))\n",
    "j = 0\n",
    "for i in range(0, 14, 1):\n",
    "    j = j+1\n",
    "    plt.subplot(14, 2, j)\n",
    "    plt.hist(xtrain[:, i], density=True)\n",
    "    j = j+1\n",
    "    plt.subplot(14, 2, j)\n",
    "    plt.hist(xtrain[:, i+13], density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T18:39:45.190216Z",
     "start_time": "2020-02-24T18:39:38.456912Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "ytrain[xtrain[:, i] == 1]\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(11):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(xtrain[:, i+26], density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:02:28.979264Z",
     "start_time": "2020-03-11T15:02:28.975112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = decomposition.PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:47:35.088405Z",
     "start_time": "2020-02-28T19:47:31.514204Z"
    }
   },
   "outputs": [],
   "source": [
    "df_feat_pca = pca.fit_transform(x_train)\n",
    "print('percent of the variance '+ str(sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:02:34.553040Z",
     "start_time": "2020-03-11T15:02:32.646893Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(x_train)\n",
    "datapoint = pca.transform(x_train)\n",
    "print('percent of the variance '+ str(sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:03:26.660001Z",
     "start_time": "2020-03-11T15:03:19.141498Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=y_train, cmap='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:04:31.508095Z",
     "start_time": "2020-03-11T15:04:30.689250Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3).fit(x_train)\n",
    "datapoint = pca.transform(x_train)\n",
    "print('percent of the variance '+ str(sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:05:45.638260Z",
     "start_time": "2020-03-11T15:05:39.240087Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(datapoint[:, 0], datapoint[:, 1], datapoint[:, 2],c  = y_train,cmap='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:20:47.566039Z",
     "start_time": "2020-03-11T15:19:04.714093Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tSNE = TSNE(n_components=2)\n",
    "\n",
    "#with 8000 data\n",
    "datapoint = tSNE.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:20:48.945007Z",
     "start_time": "2020-03-11T15:20:47.574718Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1],c  = y_train,cmap='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:34:14.788023Z",
     "start_time": "2020-03-11T15:24:25.727758Z"
    }
   },
   "outputs": [],
   "source": [
    "tSNE = TSNE(n_components=3)\n",
    "datapoint = tSNE.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:45:47.405594Z",
     "start_time": "2020-03-11T15:45:46.162550Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(datapoint[:, 0], datapoint[:, 1],\n",
    "           datapoint[:, 2], c=y_train, cmap='Set1')\n",
    "\n",
    "ax.view_init(elev=10., azim=210)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1LRmujwuKCx"
   },
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augementation with Permutation of columns of image A and image B \n",
    "As we see, the col 0-12 and col 13-25 represent respectively pic1 and pic2, however, the machine learning algorithm doesn’t necessarily has this knowledge and would consider each feature a irrelevant feature at the beginning of learning. Even after learning it may not catch this pattern. In order to cope with this problem, I firstly do the swap of pic 1 and pic2 every two lines. By doing so I improved 97% to 98%  in terms of validation score. Soon I choose another better solutaion.\n",
    "\n",
    "I make a hypothesis that the matching scores are symmetric similarities, which means the position of pic 1 and pic 2 shouldn't change the similarity measure. I have Score(pic 1 , pic 2) = Score( pic 2 , pic 1 ).\n",
    "\n",
    "Then I duplicate x_train and swap pic 1 and pic 2, and added it back into x_train. Thus I doubled the dataset. I took 9M as training set, now I have 18M data. This not only  let the algorithmes be aware of the subsets pic1 and pic2, but also added one time more useful data. By doing so I get a score around 99.8% on validation set.\n",
    "\n",
    "\n",
    "| col 0-12 | col 13-25   | matching scores  |label|\n",
    "|----       --|------|------|------|\n",
    "| PIC 1 | PIC 2   |  scores  |y|\n",
    "|------|------|------|------|\n",
    "|   PIC 2       | PIC 1| scores   |y|\n",
    "\n",
    "The result is very surprising, showing that this method is very efficient and my hypothesis is true. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:48:22.792824Z",
     "start_time": "2020-03-11T15:48:22.779224Z"
    }
   },
   "outputs": [],
   "source": [
    "# duplicate x_train\n",
    "def data_augementation(x_train,y_train):\n",
    "    z = np.copy(x_train)\n",
    "\n",
    "    # Permutation of pic 1 and pic 2\n",
    "    x_train[:, [0, 13]] = x_train[:, [13, 0]]\n",
    "    x_train[:, [1, 14]] = x_train[:, [14, 1]]\n",
    "    x_train[:, [2, 15]] = x_train[:, [15, 2]]\n",
    "    x_train[:, [3, 16]] = x_train[:, [16, 3]]\n",
    "    x_train[:, [4, 17]] = x_train[:, [17, 4]]\n",
    "    x_train[:, [5, 18]] = x_train[:, [18, 5]]\n",
    "    x_train[:, [6, 19]] = x_train[:, [19, 6]]\n",
    "    x_train[:, [7, 20]] = x_train[:, [20, 7]]\n",
    "    x_train[:, [8, 21]] = x_train[:, [21, 8]]\n",
    "    x_train[:, [9, 22]] = x_train[:, [22, 9]]\n",
    "    x_train[:, [10, 23]] = x_train[:, [23, 10]]\n",
    "    x_train[:, [11, 24]] = x_train[:, [24, 11]]\n",
    "    x_train[:, [12, 25]] = x_train[:, [25, 12]]\n",
    "\n",
    "    # stack x_train and permuted x_train\n",
    "    x_train = np.vstack((x_train, z))\n",
    "    y_train=np.hstack((y_train,y_train))\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:35.743862Z",
     "start_time": "2020-02-27T09:47:35.198617Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "- From the above distribution and correlation analysis, the column $(0,1,2,11,12,13,14,15,24,25)$ seems to have strange outliers when is far far from the $mean+-3*std$. Like the col 1, there are 0 values while mean = 0.99 and std is 0.06. \n",
    "I prefer to keep the outliers of other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:05:59.270227Z",
     "start_time": "2020-02-13T16:05:58.949864Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "EOH1SEZZHI6c",
    "outputId": "15e2f227-2cb6-42d1-b768-91f6b94512ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame(x_train, columns=['fA1', 'fA2', 'fA3', 'fA4', 'fA5', 'fA6', 'fA7', 'fA8', 'fA9', 'fA10', 'fA11',\n",
    "                                    'fA12', 'fA13', 'fB1', 'fB2', 'fB3', 'fB4', 'fB5', 'fB6', 'fB7', 'fB8', 'fB9', 'fB10', 'fB11', 'fB12', 'fB13',\n",
    "                                    'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11'])\n",
    "xdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T18:22:16.585789Z",
     "start_time": "2020-02-24T18:22:16.574777Z"
    }
   },
   "outputs": [],
   "source": [
    "# detect the data with at least one outlier\n",
    "# as default we only clean the columns 0, 1, 2, 11, 12, 13, 14, 15, 24, 25\n",
    "\n",
    "def detect_outliers(xtrain, factor=3, r=None):\n",
    "    outlier_indice = []\n",
    "    if r == None:\n",
    "        #r = range(26)\n",
    "        r = (0, 1, 2, 11, 12, 13, 14, 15, 24, 25)\n",
    "    else:\n",
    "        r = r\n",
    "\n",
    "    for i in r:\n",
    "        upper_lim = xtrain[:, i].mean()+xtrain[:, i].std()*factor\n",
    "        lower_lim = xtrain[:, i].mean()-xtrain[:, i].std()*factor\n",
    "\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_i = np.where(\n",
    "            (xtrain[:, i] < lower_lim) | (xtrain[:, i] > upper_lim))\n",
    "\n",
    "        first_list = outlier_indice\n",
    "        second_list = list(outlier_list_i)\n",
    "        # print(outlier_list_i)\n",
    "        # print(len(outlier_list_i[0]))\n",
    "        in_first = set(first_list)\n",
    "        in_second = set(outlier_list_i[0])\n",
    "        in_second_but_not_in_first = in_second - in_first\n",
    "\n",
    "        outlier_indice = first_list + list(in_second_but_not_in_first)\n",
    "\n",
    "    return outlier_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T18:22:25.839417Z",
     "start_time": "2020-02-24T18:22:25.824861Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop all the data with at least one outlier\n",
    "def drop_outliers(xtrain, ytrain, factor=3, r=None):\n",
    "    outlier_list = detect_outliers(xtrain, factor, r)\n",
    "    xtrain_without_outliers = np.delete(xtrain, outlier_list, axis=0)\n",
    "    ytrain_without_outliers = np.delete(ytrain, outlier_list, axis=0)\n",
    "    print(\"used to have train data number\", xtrain.shape[0])\n",
    "    print(\"number of outliers\", len(outlier_list))\n",
    "    print(\"number of cleaned train data\", ytrain_without_outliers.shape[0])\n",
    "    return xtrain_without_outliers, ytrain_without_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s12wEnxrg-ef"
   },
   "source": [
    "## Feature Reengineering\n",
    "I add new features like subtraction and division between 2 pictures, this may help basic models to learn better the insights from the datas. I tested on Logistic Regression, the TPR before and after adding these features are 85% and 90% ( 200K/100K : train/validation ). \n",
    "\n",
    "However, for model with large capacity, adding these new features seems unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T08:25:03.248845Z",
     "start_time": "2020-02-17T08:25:03.098968Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "43EfY-oNg-vC"
   },
   "outputs": [],
   "source": [
    "# substraction and divistion of comparable features\n",
    "substraction = x_train[:, 0:12]-x_train[:, 13:25]\n",
    "divistion = x_train[:, 13:25]/(x_train[:, 0:12]+0.00000000000001)\n",
    "x_train_new = np.c_[x_train, substraction, divistion]\n",
    "\n",
    "substraction = x_test[:, 0:12]-x_test[:, 13:25]\n",
    "divistion = x_test[:, 13:25]/(x_test[:, 0:12]+0.00000000000001)\n",
    "x_test_new= np.c_[x_test, substraction, divistion]\n",
    "\n",
    "print(x_train_new.shape)\n",
    "print(x_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T08:35:06.672646Z",
     "start_time": "2020-02-17T08:34:49.272653Z"
    }
   },
   "outputs": [],
   "source": [
    "# test on WRF\n",
    "clf0 = RandomForestClassifier(class_weight={0: 5800, 1: 1}, max_features='sqrt',\n",
    "                             n_jobs=-1, bootstrap=True,  n_estimators=100)  # , random_state=2)\n",
    "clf0.fit(x_train, y_train)\n",
    "# validation\n",
    "pred = clf0.predict(x_test)\n",
    "pred_prob = clf0.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T08:34:47.216168Z",
     "start_time": "2020-02-17T08:34:24.913648Z"
    }
   },
   "outputs": [],
   "source": [
    "# test on WRF\n",
    "clf = RandomForestClassifier(class_weight={0: 5800, 1: 1}, max_features='sqrt',\n",
    "                             n_jobs=-1, bootstrap=True,  n_estimators=100)  # , random_state=2)\n",
    "clf.fit(x_train_new, y_train)\n",
    "# validation\n",
    "pred = clf.predict(x_test_new)\n",
    "pred_prob = clf.predict_proba(x_test_new)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T18:39:45.244361Z",
     "start_time": "2020-02-24T18:23:24.072Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_\n",
    "sns.set()\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.bar(range(len(clf.feature_importances_)),clf.feature_importances_,label='model with new features',alpha=0.6)\n",
    "plt.plot(range(len(clf.feature_importances_)),clf.feature_importances_)\n",
    "plt.bar(range(len(clf0.feature_importances_)),clf0.feature_importances_,label='model without new features',alpha=0.6)\n",
    "plt.plot(range(len(clf0.feature_importances_)),clf0.feature_importances_)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Sensitive Learning and our objective\n",
    "## Analysis the objective\n",
    "Let’s consider the confusion matrix for the two-class problem. The confusion matrix is given for example. In the two-class problem, if you correctly classify something as positive, it’s called a True Positive, and it’s called a True Negative when you properly classify the negative class.  The other two possible cases\n",
    " are False Negative and False Positive. \n",
    " \n",
    "| TN          | FN   |\n",
    "|----       --|------|\n",
    "|   FP       | TP(FP=<0.0001)|\n",
    " \n",
    "- Our objective is to keep False Positive in the the zone in down left corner very low$FPR = 10^{−4}$, and maximize the TP in the down right corner. \n",
    "- However, there is a trade off between FP and TP. If we only minimise FP, the model will predict more negative, thus the True positive will decrease.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T08:39:10.855587Z",
     "start_time": "2020-02-17T08:39:10.531027Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(pred,y_test),annot=True,fmt='3.0f',cmap=\"coolwarm\",linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymetric Cost of TP and FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, we cann't assume that the cost of classifying things is\n",
    "equal. For example, we build a system to detect whether a horse with stomach pain would end up living or dying.  Let’s say someone brings a horse to us and\n",
    "asks us to predict whether the horse will live or die. We say die, and rather than delay\n",
    "the inevitable, making the animal suffer and incurring veterinary bills, they have it\n",
    "euthanized. Perhaps our prediction was wrong, and the horse would have lived. Our\n",
    "classifier is only 80% accurate, after all. If we predicted this incorrectly, then an expensive animal would have been destroyed, not to mention that a human was emotionally\n",
    "attached to the animal. \n",
    "\n",
    "\n",
    "In our case, let's image it's a face recognition for a bank, if we have false negative, then a client could be denied access, but if we have false positive, then we may let in some potential malefactors. To avoid the danger of let pass the wrong user, we only allow a false positive rate of $10^{−4}$. \n",
    "\n",
    "\n",
    "With these definitions we can define some new metrics that are more useful than\n",
    "error rate when detection of one class is more important than another class. For example, we can use $Precision = TP/(TP+FP)$ and $Recall = TP/(TP+FN)$. However, why not use directly the given criterion as objective and evaluation function?\n",
    "\n",
    "In our case, the performance criterion is TPR for the value of $FPR = 10^{−4}$, that's to say we want to maximize the $Recall = TP/(TP+FN)$ while keeping Precision higher than a threshold  $Precison = TP/(TP+FP) \\approx 99.99\\%$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T17:28:31.967061Z",
     "start_time": "2020-03-11T17:28:31.956389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the performance\n",
    "\n",
    "def true_positive_rate(yvalid):\n",
    "    yvalid_scoreordered = y_test[np.argsort(yvalid)]\n",
    "    #print(yvalid_scoreordered)\n",
    "    N = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 0)\n",
    "    P = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    for i in range(nrows_valid - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "        if (FP / N > 10**-4):\n",
    "            FP = FP - 1\n",
    "            break\n",
    "    print(\"For the smallest FPR <= 10^-4 (i.e., \",\n",
    "          FP / N, \") TPR = \", TP / P, \".\", sep=\"\")\n",
    "\n",
    "    #print(\"cohen_kappa_score :  \", cohen_kappa_score(y_test, yvalid), sep=\"\")\n",
    "\n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tool used for measuring classification imbalance is the ROC curve. ROC\n",
    "stands for receiver operating characteristic, and it was first used by electrical engineers building radar systems during World War II.  The ROC curve shows how the two rates change as the threshold changes. \n",
    "\n",
    "In order to plot the ROC I need the classifier to give a numeric score of how\n",
    "positive or negative each instance is. Most classifiers give this to you. \n",
    "The input to the sigmoid in logistic regression is a numeric value. AdaBoost and SVMs\n",
    "both compute a numeric value that’s input to the predic_proba() function. All of these values\n",
    "can be used to rank how strong the prediction of a given classifier is. To build the ROC\n",
    "curve, I first sort the instances by their prediction strength. I start with the lowest\n",
    "ranked instance and predict everything below this to be in the negative class and everything above this to be the positive class. This corresponds to the point 1.0,1.0. I move\n",
    "to the next item in the list, and if that is the positive class, I move the true positive rate,\n",
    "but if that instance is in the negative class, I change the true negative rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T17:39:44.245010Z",
     "start_time": "2020-03-11T17:39:44.235551Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ROC(yvalid):\n",
    "    yvalid_scoreordered = y_test[np.argsort(yvalid)]\n",
    "    #print(yvalid_scoreordered)\n",
    "    N = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 0)\n",
    "    P = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FP_n=[]\n",
    "    TP_n=[]\n",
    "    for i in range(nrows_valid - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "        FP_n.append(FP)\n",
    "        TP_n.append(TP)\n",
    "    sns.set()   \n",
    "    plt.scatter(TP_n,FP_n,)\n",
    "    plt.axvline(x= N * 10**-4)\n",
    "    plt.xlabel('False positive')\n",
    "    plt.ylabel('True positive')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T17:39:56.035511Z",
     "start_time": "2020-03-11T17:39:50.067573Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(x_train, y_train)\n",
    "yvalid=clf.predict_proba(x_test)[:, 0]\n",
    "ROC(yvalid)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Sensitive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T09:31:27.960215Z",
     "start_time": "2020-02-17T09:31:27.952376Z"
    }
   },
   "source": [
    "### Manipulating the classifier’s decision with a cost function \n",
    "#### Weighted Random Forest\n",
    "\n",
    "Besides tuning the thresholds of our classifier, there are other approaches\n",
    "e to aid with uneven classification costs. One such method is known as costsensitive learning. \n",
    "\n",
    "For random forest I use the parameter of class_weight dict. If not given, all classes are supposed to have weight one.\n",
    "Weights associated with classes in the form {class_label: weight}. The best I find with my hyperparameter tuning for Weighted Random Forest is around class_weight={0: 5000, 1: 1} (best TPR = 0.9464351139444567) (Before data augementation)\n",
    "                                 \n",
    "#### Boosting \n",
    "\n",
    "For boosting methodes, we can define custom objective function and custom evaluation. I will develop in section 5.1 and 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xebFmaidHC1d"
   },
   "source": [
    "\n",
    "## Experimental Models\n",
    "### Baseline - LogisticRegression - with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 26 features are mades directly from the 2 pictures, however, these features has few correlation that we can exploite directly from. I tested some features engineering on logistic regression model without any strong conviction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:01:27.299166Z",
     "start_time": "2020-02-28T20:01:23.630404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 Train the classifier with all features\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "pred_prob = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "print(\"Train the classifier with all features\")\n",
    "true_positive_rate(pred_prob)\n",
    "\n",
    "# 2 Train the classifier with only features of 1-26 cols\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(x_train[:, 1:25], y_train)\n",
    "\n",
    "pred = clf.predict(x_test[:, 1:25])\n",
    "pred_prob = clf.predict_proba(x_test[:, 1:25])[:, clf.classes_ == 1][:, 0]\n",
    "print(\"Train the classifier with only features of 1-26 cols\")\n",
    "true_positive_rate(pred_prob)\n",
    "\n",
    "# 3 Train the classifier features of 27-36 cols\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(x_train[:, 26:], y_train)\n",
    "\n",
    "pred = clf.predict(x_test[:, 26:])\n",
    "pred_prob = clf.predict_proba(x_test[:, 26:])[:, clf.classes_ == 1][:, 0]\n",
    "print(\"Train the classifier with features of 27-36 cols\")\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T22:01:15.922268Z",
     "start_time": "2020-02-18T22:01:15.329971Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 Train the classifier without outliers of 1-26 cols- with all features\n",
    "xtrain_without_outliers, ytrain__without_outliers = drop_outliers(\n",
    "    x_train, y_train, factor=3, r=(0, 1, 2, 11, 12, 13, 14, 15, 24, 25))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(xtrain_without_outliers, ytrain__without_outliers)\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "pred_prob = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "print(\"Train the classifier without outliers of 1-26 cols- with all features\")\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T15:31:34.067705Z",
     "start_time": "2020-02-16T15:31:33.653628Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(pred, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:14:27.611250Z",
     "start_time": "2020-02-17T11:14:24.887805Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 Train the classifier without outliers of 1-26 cols- with all features\n",
    "xtrain_without_outliers, ytrain__without_outliers = drop_outliers(\n",
    "    x_train, y_train, factor=3, r=(0, 1, 2, 11, 12, 13, 14, 15, 24, 25))\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',class_weight={0: 5000, 1: 1})\n",
    "clf.fit(xtrain_without_outliers, ytrain__without_outliers)\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "pred_prob = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "print(\"Train the classifier without outliers of 1-26 cols- with all features\")\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression  - drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:27:08.359347Z",
     "start_time": "2020-02-16T17:27:06.443999Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_without_outliers,ytrain__without_outliers=drop_outliers(x_train,y_train ,factor=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:27:18.523569Z",
     "start_time": "2020-02-16T17:27:11.685319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier on a part of the data set\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(xtrain_without_outliers,ytrain__without_outliers)\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "pred_prob = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression - with 61 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:46:26.272780Z",
     "start_time": "2020-02-16T19:45:56.586946Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='saga',class_weight={0: 10000, 1: 1},n_jobs=-1)\n",
    "clf.fit(x_train_new[:,27:],y_train)\n",
    "\n",
    "pred = clf.predict(x_test_new[:,27:])\n",
    "pred_prob = clf.predict_proba(x_test_new[:,27:])[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRF Weighted Random Forest\n",
    "Another approach to make random forest more suitable for learning from extremely imbalanced data follows the idea of cost sensitive learning. Since the RF classifier tends to be biased towards the majority class, we\n",
    "shall place a heavier penalty on misclassifying the minority class. We assign a weight to each class, with the\n",
    "minority class given larger weight (i.e., higher misclassification cost).\n",
    "\n",
    "\n",
    "The class weights are incorporated\n",
    "into the RF algorithm in two places. In the tree induction procedure, class weights are used to weight\n",
    "the Gini criterion for finding splits. In the terminal nodes of each tree, class weights are again taken into\n",
    "consideration. The class prediction of each terminal node is determined by “weighted majority vote”; i.e.,\n",
    "the weighted vote of a class is the weight for that class times the number of cases for that class at the\n",
    "terminal node. The final class prediction for RF is then determined by aggregatting the weighted vote from\n",
    "each individual tree, where the weights are average weights in the terminal nodes. Class weights are an\n",
    "essential tuning parameter to achieve desired performance. \n",
    "\n",
    "The out-of-bag estimate of the accuracy from\n",
    "RF can be used to select weights. This method, Weighted Random Forest (WRF), is incorporated in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:57:47.591059Z",
     "start_time": "2020-02-16T19:52:55.736628Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fhLmcU2u-OUX"
   },
   "outputs": [],
   "source": [
    "# Train the classifier on a part of the data set\n",
    "clf = RandomForestClassifier(class_weight={\n",
    "                             0: 6000, 1: 1}, max_features='sqrt',\n",
    "                             n_jobs=-1, bootstrap=True,  n_estimators=100)  # , random_state=2)\n",
    "\n",
    "clf.fit(x_train_new, y_train)\n",
    "\n",
    "# Check its on another part of the data set\n",
    "pred = clf.predict(x_test_new)\n",
    "pred_prob = clf.predict_proba(x_test_new)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:57:48.181549Z",
     "start_time": "2020-02-16T19:57:47.600121Z"
    }
   },
   "outputs": [],
   "source": [
    "#pred = clf.predict(x_test)\n",
    "sns.heatmap(confusion_matrix(pred, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:44:05.445929Z",
     "start_time": "2020-02-16T17:44:04.629404Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_\n",
    "plt.plot(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:57:48.861968Z",
     "start_time": "2020-02-16T19:57:48.185861Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_\n",
    "plt.plot(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T19:57:49.009974Z",
     "start_time": "2020-02-16T19:57:48.865954Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, 'WRF6000_1_new.joblib')  #name rule : WRF1000_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T16:33:02.243413Z",
     "start_time": "2020-02-16T16:33:02.146787Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = load('WRF20000_1.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the best class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T23:48:42.915355Z",
     "start_time": "2020-02-16T20:10:29.928648Z"
    }
   },
   "outputs": [],
   "source": [
    "TPR = []\n",
    "Feature_importance = []\n",
    "r = range(1000, 8000, 200)\n",
    "for i in r:\n",
    "\n",
    "    clf = RandomForestClassifier(class_weight={\n",
    "                                 0: i, 1: 1}, max_features='sqrt', n_jobs=-1, bootstrap=True, random_state=2, n_estimators=100)\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Check its on another part of the data set\n",
    "    yvalid = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "    yvalid_scoreordered = y_test[np.argsort(yvalid)]\n",
    "\n",
    "    N = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 0)\n",
    "    P = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    for i in range(nrows_valid - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "        if (FP / N > 10**-4):\n",
    "            FP = FP - 1\n",
    "            break\n",
    "    TPR.append(TP/P)\n",
    "    Feature_importance.append(clf.feature_importances_)\n",
    "\n",
    "print(\"For the smallest FPR <= 10^-4, the best TPR = \", np.max(TPR), \".\", sep=\"\")\n",
    "sns.set()\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(r, TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T13:46:22.843776Z",
     "start_time": "2020-02-16T13:46:22.290682Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T12:56:32.727052Z",
     "start_time": "2020-02-17T12:54:36.306033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier on a part of the data set\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=1, class_weight={0: 1000, 1: 1})  # class_weight={0: 800, 1: 1}\n",
    "clf = AdaBoostClassifier(\n",
    "    base_estimator=None, n_estimators=100, learning_rate=0.8, algorithm='SAMME.R', random_state=None)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Check its on another part of the data set\n",
    "pred = clf.predict(x_test)\n",
    "pred_prob = clf.predict_proba(x_test)[:, clf.classes_ == 1][:, 0]\n",
    "\n",
    "true_positive_rate(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T14:50:46.617827Z",
     "start_time": "2020-02-16T14:50:46.588092Z"
    }
   },
   "outputs": [],
   "source": [
    "true_positive_rate(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T14:56:14.277682Z",
     "start_time": "2020-02-16T14:56:12.915337Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(pred, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RAW8cEFxSNif"
   },
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:47:03.000372Z",
     "start_time": "2020-02-13T16:46:58.875993Z"
    }
   },
   "source": [
    "XGboost use a second ordre Taylor approximation, so it requests the gradient and the hessien in its cost function. For binary classification, we use a log loss:\n",
    "$$𝐿=−𝑦ln𝑝−𝛽(1−𝑦)ln(1−𝑝)$$ $p$ as the probability estimated by sigmoid function. $𝛽$ is the multiplier factor to increase the weight of FP loss.\n",
    "\n",
    "In order to penalise False Positive, we put a penalty Beta on the FP, we can calculate the gradent as: \n",
    "\n",
    "$$grad =\\frac{∂𝐿}{∂𝑥}=\\frac{∂𝐿}{∂𝑝}\\frac{∂𝑝}{∂𝑥}=𝑝(𝛽+𝑦−𝛽𝑦)−𝑦 $$,\n",
    "\n",
    "and hessien as:\n",
    "\n",
    "$$hess =∂2𝐿∂𝑥2=𝑝(1−𝑝)(𝛽+𝑦−𝛽𝑦)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T13:31:43.655366Z",
     "start_time": "2020-02-17T13:31:43.072686Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "D_train = xgb.DMatrix(x_train, label=y_train)\n",
    "D_valid = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:29:58.070569Z",
     "start_time": "2020-02-17T17:29:58.066169Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def weighted_logloss(dtrain, y_hat):\n",
    "    y = dtrain\n",
    "    p = y_hat\n",
    "    beta = 500\n",
    "    grad = p * (beta + y - beta*y) - y\n",
    "    hess = p * (1 - p) * (beta + y - beta*y)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T19:00:51.145461Z",
     "start_time": "2020-02-17T19:00:51.078239Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_loss_function(y, pred):\n",
    "    beta = 0.01\n",
    "    p = 1. / (1 + np.exp(-pred))\n",
    "    grad = p * ((beta - 1) * y + 1) - beta * y\n",
    "    hess = ((beta - 1) * y + 1) * p * (1.0 - p)\n",
    "\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T19:03:41.258052Z",
     "start_time": "2020-02-17T19:01:28.150085Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_set = [(x_test, y_test)]\n",
    "\n",
    "param = {'max_depth': 28, 'scale_pos_weight': 0.01, 'eta': 0.1,\n",
    "         'objective': 'binary:logistic', 'eval_metric': 'error@0.8'}\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=20, n_estimators=40, learning_rate=0.05,\n",
    "                        objective=custom_loss_function,\n",
    "                        eval_metric=\"logloss\", eval_set=eval_set, verbose_eval=True,\n",
    "                        early_stopping_rounds=10,\n",
    "                        scale_pos_weight=0.07, random_state=2, verbosity=2, n_jobs=-1,).fit(x_train, y_train)\n",
    "preds_proba = gbm.predict(x_test)\n",
    "\n",
    "true_positive_rate(preds_proba)\n",
    "\n",
    "sns.heatmap(confusion_matrix(preds_proba, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T16:39:38.176425Z",
     "start_time": "2020-02-17T16:39:32.625398Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_proba = gbm.predict(x_test)\n",
    "\n",
    "true_positive_rate(preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T15:57:50.848851Z",
     "start_time": "2020-02-17T15:57:50.576051Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(y_test, preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:04:30.242711Z",
     "start_time": "2020-02-17T14:04:29.092367Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(preds__proba, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T13:30:41.181296Z",
     "start_time": "2020-02-17T13:30:40.389249Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:44:37.364169Z",
     "start_time": "2020-02-13T17:44:17.698973Z"
    }
   },
   "outputs": [],
   "source": [
    "ax= xgb.plot_tree(bst, num_trees=1)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(18, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "    \"eta\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "    \"max_depth\": [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"colsample_bytree\": [0.3, 0.4, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wdUYCiGHBHF"
   },
   "source": [
    "# (customized) Lightgbm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T19:52:16.873586Z",
     "start_time": "2020-02-18T19:52:16.846392Z"
    }
   },
   "source": [
    "## Custom Loss Function, penalty for FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost use a second ordre Taylor approximation, very alike to Xgboost, light gbm also request the gradient and the hessien in its cost function. For binary classification, we use a log loss:\n",
    "$$𝐿=−𝑦ln𝑝−𝛽(1−𝑦)ln(1−𝑝)$$ $p$ as the probability estimated by sigmoid function. $𝛽$ is the multiplier factor to increase the weight of FP loss.\n",
    "\n",
    "In order to penalise False Positive, we put a penalty Beta on the FP, we can calculate the gradient as: \n",
    "\n",
    "$$grad =\\frac{∂𝐿}{∂𝑥}=\\frac{∂𝐿}{∂𝑝}\\frac{∂𝑝}{∂𝑥}=𝑝(𝛽+𝑦−𝛽𝑦)−𝑦 $$,\n",
    "\n",
    "and hessien as:\n",
    "\n",
    "$$hess =∂2𝐿∂𝑥2=𝑝(1−𝑝)(𝛽+𝑦−𝛽𝑦)$$\n",
    "\n",
    " Customizing the training loss in LightGBM requires defining a function that takes in two arrays, the targets and their predictions. In turn, the function should return two arrays of the gradient and hessian of each observation. As noted above, we need to use calculus to derive gradient and hessian and then implement it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T08:30:40.648516Z",
     "start_time": "2020-02-19T08:30:40.635892Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_loss_function(preds, train_data):\n",
    "    # pred = raw value before logistic transformation <- 0 ->\n",
    "    y = train_data.get_label()\n",
    "    pred = preds\n",
    "    beta = 5\n",
    "\n",
    "    #p = 1. / (1. + np.exp(-pred)) to avoid exp overflow:\n",
    "    p = np.where(preds >= 0, 1.0 / (1+np.exp(-preds)),\n",
    "                 np.exp(preds)/(1+np.exp(preds)))\n",
    "    grad = (p * (beta + y - beta * y) - y)\n",
    "    hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:18:26.768465Z",
     "start_time": "2020-02-18T16:18:26.757753Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# test the cost function\n",
    "y = np.asarray([1, 1, 0, 0])\n",
    "pred = np.asarray([1, 0, 1, 0])\n",
    "beta = 0.1\n",
    "\n",
    "p = 1. / (1 + np.exp(-pred))\n",
    "print(\"residu\", pred-y)\n",
    "print(\"p\", p)\n",
    "\n",
    "grad = (p * (beta + y - beta * y) - y)\n",
    "\n",
    "hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "print(\"grad\", grad)\n",
    "print(\"hess\", hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:06:38.751920Z",
     "start_time": "2020-02-18T20:06:38.739548Z"
    }
   },
   "source": [
    "## Custom Eval Metric TPR\n",
    "\n",
    "Validation loss: This is the function that we use to evaluate the performance of our trained model on unseen data. This is often not the same as the training loss. For example, in the case of a classifier, this is often the area under the curve of the receiver operating characteristic (ROC) — though this is never directly optimized, because it is not differentiable. This is often called the “performance or evaluation metric”. The validation loss is often used to tune hyper-parameters. \n",
    "\n",
    "Because it doesn’t have as many functional requirements like the training loss does, the validation loss can be non-convex, non-differentiable, and discontinuous, we can use our Specific True Positive Rate (conditionned by FP < 10e-4) as validation loss. The validation loss in LightGBM is called metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T13:58:01.939719Z",
     "start_time": "2020-02-20T13:58:01.862756Z"
    }
   },
   "outputs": [],
   "source": [
    "def TPR_eval(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    yvalid_scoreordered = labels[np.argsort(preds)]\n",
    "\n",
    "    N = np.sum(y_test == 0)\n",
    "    P = np.sum(y_test == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "\n",
    "    for i in range(len(labels) - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "            #print(\"True Positive\", TP, \"False Positive\", FP)\n",
    "        if (FP / N > 10**-4):\n",
    "            FP = FP - 1\n",
    "            break\n",
    "\n",
    "    # is_higher_better=True\n",
    "    return \"###TPR(FPR<0.0001)###:\", TP/P, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T20:23:13.688148Z",
     "start_time": "2020-02-28T20:23:13.625197Z"
    }
   },
   "outputs": [],
   "source": [
    "# a very with more verbose for observation and debug\n",
    "def TPR_eval_verbose(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    yvalid_scoreordered = y_test[np.argsort(preds)]\n",
    "    preds_scoreordered = preds[np.argsort(preds)]\n",
    "\n",
    "    N = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 0)\n",
    "    P = np.sum(ytrain[nrows_train:(nrows_train + nrows_valid)] == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    TP_total = 0\n",
    "    FN_total = 0\n",
    "    TN_total = 0\n",
    "    FP_total = 0\n",
    "    for i in range(len(labels) - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "            print(\"True Positive\", TP, \"False Positive\", FP)\n",
    "        if (FP / N > 10**-4):\n",
    "            FP = FP - 1\n",
    "            break\n",
    "    print(\"True Positive\", TP, \"False Positive\", FP)\n",
    "    # added\n",
    "    for i in range(nrows_valid - 1, -1, -1):\n",
    "        # print(preds_scoreordered[i])\n",
    "        if (yvalid_scoreordered[i] == 0):\n",
    "            if preds_scoreordered[i] < 0.5:\n",
    "                TN_total = TN_total+1\n",
    "\n",
    "            else:\n",
    "                FP_total = FP_total+1\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            if preds_scoreordered[i] > 0.5:\n",
    "                TP_total = TP_total+1\n",
    "            else:\n",
    "                FN_total = FN_total+1\n",
    "\n",
    "    #print(\"cohen_kappa_score :  \", cohen_kappa_score(y_test, yvalid), sep=\"\")\n",
    "    print(\"____________________________\")\n",
    "    print(\"FP\", FP_total, \"FN:\", FN_total, \"FN/FP:\", FN_total/FP_total)\n",
    "    return \"###TPR(FPR<0.0001)###:\", TP/P,  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T15:42:44.545610Z",
     "start_time": "2020-02-18T15:42:44.525186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other eval metric\n",
    "def accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    return 'accuracy', np.mean(labels == (preds > 0.5)), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T19:16:51.171783Z",
     "start_time": "2020-02-18T19:13:16.456784Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#run with cumstom loss and eval\n",
    "lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(data=x_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    # 'objective' = custom_loss_function,\n",
    "    # 'metric':{'12','auc','binary_logloss'},\n",
    "    'num_leaves': 600,\n",
    "    'num_trees': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    # 'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'scale_pos_weight': 0.0969,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                num_boost_round=500,\n",
    "                fobj=custom_loss_function,\n",
    "                # feval=binary_error,\n",
    "                feval=TPR_eval,\n",
    "                #learning_rates=lambda iter: 0.05 * (0.99 ** iter),\n",
    "                #feval=lambda preds, train_data: [accuracy(preds, train_data),TPR_eval(preds, train_data)],\n",
    "                early_stopping_rounds=50)\n",
    "\n",
    "print(gbm.best_score)\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:13:35.312294Z",
     "start_time": "2020-02-18T16:13:34.079794Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "gbm.best_score\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "true_positive_rate(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:03:45.902297Z",
     "start_time": "2020-02-18T20:03:44.768714Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_l = 1. / (1. + np.exp(-y_pred))\n",
    "y_scoreordered = y_test[np.argsort(pred_l)]\n",
    "p_scoreordered = y_pred[np.argsort(pred_l)]\n",
    "p_thershlod = p_scoreordered[-13000]\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "FN = []\n",
    "FP = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] <= 0 and y_test[i] == 1:\n",
    "        FN.append(y_pred[i])\n",
    "\n",
    "    if y_pred[i] > 1 and y_test[i] == 0:\n",
    "        FP.append(y_pred[i])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "sns.distplot(y_pred, hist=True, kde=False,\n",
    "             bins=40, color='darkblue',\n",
    "             hist_kws={'edgecolor': 'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "plt.axvline(0)\n",
    "plt.axvline(x=p_thershlod)\n",
    "plt.scatter(FN, np.ones(len(FN)))\n",
    "plt.scatter(FP, np.ones(len(FP)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:42:35.148300Z",
     "start_time": "2020-02-18T16:42:34.149341Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_heatmap=np.where(y_pred>=0,1,0)\n",
    "sns.heatmap(confusion_matrix(preds_heatmap, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T16:42:33.780354Z",
     "start_time": "2020-02-18T16:42:29.016521Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "#ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "#plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(gbm, max_num_features=40)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.show()\n",
    "\n",
    "print('Plotting split value histogram...')\n",
    "ax = lgb.plot_split_value_histogram(gbm, feature='Column_29', bins='auto')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 54th tree...')  # one tree use categorical feature to split\n",
    "ax = lgb.plot_tree(gbm, tree_index=53, figsize=(15, 15), show_info=['split_gain'])\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 54th tree with graphviz...')\n",
    "graph = lgb.create_tree_digraph(gbm, tree_index=53, name='Tree54')\n",
    "#graph.render(view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T08:44:50.157372Z",
     "start_time": "2020-02-19T08:44:34.743454Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_without_outliers, ytrain_without_outliers = drop_outliers(xtrain, ytrain, factor=3,r=(0,1,2,11,12,13,14,15,24,25))\n",
    "\n",
    "def custom_loss_function(preds, train_data):\n",
    "    # pred = raw value before logistic transformation <- 0 ->\n",
    "    y = train_data.get_label()\n",
    "    pred = preds\n",
    "    beta = 5\n",
    "    p = 1. / (1. + np.exp(-pred))\n",
    "    grad = (p * (beta + y - beta * y) - y)\n",
    "    hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(data=xtrain_without_outliers, label=ytrain_without_outliers)\n",
    "lgb_eval = lgb.Dataset(data=x_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    #'objective': 'binary',\n",
    "    #'metric':{'12','auc','binary_logloss','TPR_eval'},\n",
    "    'num_leaves': 600,\n",
    "    'learning_rate': 0.05,\n",
    "    # 'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'scale_pos_weight': 0.0969,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "evals_result = {}\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                num_boost_round=50,\n",
    "                #fobj=custom_loss_function,\n",
    "                # feval=binary_error,\n",
    "                #feval=TPR_eval,\n",
    "                feval=TPR_eval_verbose,\n",
    "                evals_result=evals_result,\n",
    "                #learning_rates=lambda iter: 0.05 * (0.99 ** iter),\n",
    "                #feval=lambda preds, train_data: [accuracy(preds, train_data),TPR_eval(preds, train_data)],\n",
    "                early_stopping_rounds=50)\n",
    "\n",
    "print(gbm.best_score)\n",
    "print('Starting predicting...')\n",
    "\n",
    "# predict\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "print(y_pred)\n",
    "\n",
    "# true_positive_rate(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:48:02.273072Z",
     "start_time": "2020-02-18T20:48:01.271031Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_heatmap=np.where(y_pred>=0.5,1,0)\n",
    "sns.heatmap(confusion_matrix(preds_heatmap, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:44:37.717991Z",
     "start_time": "2020-02-18T20:44:36.501862Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_l = 1. / (1. + np.exp(-y_pred))\n",
    "y_scoreordered = y_test[np.argsort(pred_l)]\n",
    "p_scoreordered = y_pred[np.argsort(pred_l)]\n",
    "p_thershlod=p_scoreordered[-13000]\n",
    "\n",
    "FN=[]\n",
    "FP=[]\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "        if y_pred[i] <= 0 and y_test[i] == 1:\n",
    "            FN.append(y_pred[i])\n",
    "            \n",
    "        if y_pred[i] > 1 and y_test[i] == 0:\n",
    "            FP.append(y_pred[i])\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "sns.distplot(y_pred, hist=True, kde=False, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "plt.axvline(0)\n",
    "plt.scatter(FN,np.ones(len(FN)))\n",
    "plt.scatter(FP,np.ones(len(FP)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T20:28:26.693012Z",
     "start_time": "2020-02-18T20:27:48.404188Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "ax = lgb.plot_metric( evals_result, metric='auc')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(gbm, max_num_features=40)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.show()\n",
    "\n",
    "print('Plotting split value histogram...')\n",
    "ax = lgb.plot_split_value_histogram(gbm, feature='Column_29', bins='auto')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 54th tree...')  # one tree use categorical feature to split\n",
    "ax = lgb.plot_tree(gbm, tree_index=53, figsize=(15, 15), show_info=['split_gain'])\n",
    "plt.show()\n",
    "\n",
    "print('Plotting 54th tree with graphviz...')\n",
    "graph = lgb.create_tree_digraph(gbm, tree_index=53, name='Tree54')\n",
    "#graph.render(view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T21:08:17.929804Z",
     "start_time": "2020-02-18T21:08:15.633489Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model to file\n",
    "gbm.save_model('model_1.txt')\n",
    "\n",
    "print('Dumping model to JSON...')\n",
    "# dump model to JSON (and save to file)\n",
    "model_1_json = gbm.dump_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T09:12:11.861007Z",
     "start_time": "2020-02-19T09:12:11.844501Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def TPR_eval_cv( preds,train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    yvalid_scoreordered = labels[np.argsort(preds)]\n",
    "\n",
    "    N = np.sum(y_test == 0)\n",
    "    P = np.sum(y_test == 1)\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "\n",
    "    for i in range(math.floor(nrows_valid/k) - 1, -1, -1):\n",
    "        if (yvalid_scoreordered[i] == 1):\n",
    "            TP = TP + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "            #print(\"True Positive\", TP, \"False Positive\", FP)\n",
    "        if (FP / N > 10**-4):\n",
    "            FP = FP - 1\n",
    "            break\n",
    "\n",
    "    return \"###TPR(FPR<0.0001)###:\", TP/P, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T09:13:50.889259Z",
     "start_time": "2020-02-19T09:13:26.300577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "nrows_train = 1000000\n",
    "nrows_valid = 100000\n",
    "train_valide_n=nrows_train + nrows_valid\n",
    "xtrain = np.loadtxt('/content/drive/My Drive/DataChallenge2020MSBGD/xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "ytrain = np.loadtxt('/content/drive/My Drive/DataChallenge2020MSBGD/ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_valid)\n",
    "ytrain = np.array(ytrain).reshape(nrows_train + nrows_valid)\n",
    "\n",
    "x_train=xtrain[:nrows_train]\n",
    "y_train=ytrain[:nrows_train]\n",
    "x_test=xtrain[nrows_train:(nrows_train + nrows_valid)] \n",
    "y_test=ytrain[nrows_train:(nrows_train + nrows_valid)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T08:47:35.880391Z",
     "start_time": "2020-02-19T08:47:35.862298Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T18:47:13.788141Z",
     "start_time": "2020-02-18T18:06:54.803352Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "TPR = []\n",
    "Feature_importance =[]\n",
    "\n",
    "r = range(1, 14, 1)\n",
    "global i\n",
    "i=0\n",
    "for t in range(1, 14, 1):\n",
    "    i=i+0.5\n",
    "    def custom_loss_function(preds, train_data):\n",
    "        # pred = raw value before logistic transformation <- 0 ->\n",
    "        y = train_data.get_label()\n",
    "        pred = preds\n",
    "        beta = i\n",
    "        p = 1. / (1. + np.exp(-pred))\n",
    "        grad = (p * (beta + y - beta * y) - y)\n",
    "        hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "        return grad, hess\n",
    "\n",
    "    lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "    lgb_eval = lgb.Dataset(data=x_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "    params = {\n",
    "          'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 600,\n",
    "    'num_trees': 400,\n",
    "    'learning_rate': 0.05,\n",
    "    'bagging_fraction': 0.9,\n",
    "     'scale_pos_weight':0.0969,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "    }\n",
    "\n",
    "    print(i,'----  Starting training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,lgb_train,valid_sets=lgb_eval,\n",
    "                    num_boost_round=30,\n",
    "                    fobj=custom_loss_function,\n",
    "                    # feval=binary_error,\n",
    "                    feval=TPR_eval,\n",
    "                    #learning_rates=lambda iter: 0.05 * (0.99 ** iter),\n",
    "                    early_stopping_rounds=50)\n",
    "\n",
    "    print(gbm.best_score)\n",
    "    print('Starting predicting...')\n",
    "    # predict\n",
    "    y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "     # Check its on another part of the data set\n",
    "    yvalid = y_pred\n",
    "    yvalid_scoreordered = y_test[np.argsort(yvalid)]\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T17:13:56.167863Z",
     "start_time": "2020-02-18T17:13:54.884805Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(preds_heatmap, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual tuning with all dataset\n",
    "I split the data into 9M data train set and the rest for validation set. With the permuation and data augementation the trainning set become 18M. I mainly did the tuning on google Colab.\n",
    "\n",
    "Finally I used all data including \"outliers\", I consider these \"noise\" may help to improve the robustness of the model, and the lightgbm are power enough to handle some noises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augementation , beta=2, n_leaves=1000, max_depth=10 \n",
    "def custom_loss_function(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    beta = 2\n",
    "    p = np.where(preds >= 0, 1.0 / (1+np.exp(-preds)),\n",
    "                 np.exp(preds)/(1+np.exp(preds)))\n",
    "    grad = (p * (beta + y - beta * y) - y)\n",
    "    hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(data=x_train,   label=y_train)\n",
    "lgb_eval = lgb.Dataset(data=x_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    # 'objective' = custom_loss_function,\n",
    "    'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary',\n",
    "    'num_leaves': 1000,\n",
    "    'max_depth': 10, 'learning_rate': 0.1,\n",
    "    # 'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'scale_pos_weight': 0.0747, 'bagging_freq': 5, 'verbose': 0,   'lambda_l1': 0.1,   'lambda_l2': 0.01,\n",
    "}\n",
    "evals_result = {}\n",
    "print('Starting training...')\n",
    "gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                num_boost_round=1500,\n",
    "                feval=TPR_eval,\n",
    "                fobj=custom_loss_function,\n",
    "                verbose_eval=10, evals_result=evals_result,\n",
    "                #learning_rates=lambda iter: 0.10 * (0.996 ** iter),\n",
    "                #feval=lambda preds, train_data: [accuracy(preds, train_data),TPR_eval(preds, train_data)],\n",
    "                early_stopping_rounds=50)\n",
    "\n",
    "print(gbm.best_score)\n",
    "print('Starting predicting...')\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)def custom_loss_function(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    beta = 2\n",
    "    p = np.where(preds >= 0, 1.0 / (1+np.exp(-preds)),\n",
    "                 np.exp(preds)/(1+np.exp(preds)))\n",
    "    grad = (p * (beta + y - beta * y) - y)\n",
    "    hess = p*(1.0 - p) * (beta + y - beta * y)\n",
    "    return grad, hess\n",
    "\n",
    "lgb_train = lgb.Dataset(data=x_train,   label=y_train)\n",
    "lgb_eval = lgb.Dataset(data=x_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    # 'objective' = custom_loss_function,\n",
    "    'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary',\n",
    "    'num_leaves': 1000,\n",
    "    'max_depth': 10, 'learning_rate': 0.1,\n",
    "    # 'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'scale_pos_weight': 0.0747, 'bagging_freq': 5, 'verbose': 0,   'lambda_l1': 0.1,   'lambda_l2': 0.01,\n",
    "}\n",
    "evals_result = {}\n",
    "print('Starting training...')\n",
    "gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval,\n",
    "                num_boost_round=1500,\n",
    "                feval=TPR_eval,\n",
    "                fobj=custom_loss_function,\n",
    "                verbose_eval=10, evals_result=evals_result,\n",
    "                #learning_rates=lambda iter: 0.10 * (0.996 ** iter),\n",
    "                #feval=lambda preds, train_data: [accuracy(preds, train_data),TPR_eval(preds, train_data)],\n",
    "                early_stopping_rounds=50)\n",
    "\n",
    "print(gbm.best_score)\n",
    "print('Starting predicting...')\n",
    "y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1020]\tvalid_0's ###TPR(FPR<0.0001)###:: 0.997269\n",
    "total scaned  54855 TP 54780 False Negative 150 FP 74\n",
    "total scaned  54855 TP 54780 False Negative 150 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54855 TP 54780 False Negative 150 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "[1030]\tvalid_0's ###TPR(FPR<0.0001)###:: 0.997251\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54854 TP 54779 False Negative 151 FP 74\n",
    "total scaned  54855 TP 54780 False Negative 150 FP 74\n",
    "total scaned  54855 TP 54780 False Negative 150 FP 74\n",
    "[1040]\tvalid_0's ###TPR(FPR<0.0001)###:: 0.997269\n",
    "total scaned  54856 TP 54781 False Negative 149 FP 74\n",
    "total scaned  54856 TP 54781 False Negative 149 FP 74\n",
    "Early stopping, best iteration is:\n",
    "[992]\tvalid_0's ###TPR(FPR<0.0001)###:: 0.997287\n",
    "defaultdict(<class 'dict'>, {'valid_0': {'###TPR(FPR<0.0001)###:': 0.9972874567631531}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_heatmap=np.where(y_pred>=0.5,1,0)\n",
    "sns.heatmap(confusion_matrix(preds_heatmap, y_test), annot=True,\n",
    "            fmt='3.0f', cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking \n",
    "## Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "xtest = np.loadtxt(\n",
    "    '/content/drive/My Drive/DataChallenge2020MSBGD/xtest_challenge.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T17:19:39.006370Z",
     "start_time": "2020-02-20T17:19:37.605041Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the models\n",
    "clf1 = lgb.Booster(\n",
    "    model_file='/content/drive/My Drive/DataChallenge2020MSBGD/model_save/final5.txt')\n",
    "y_pred1 = clf1.predict(xtest, num_iteration=clf1.best_iteration)\n",
    "# true_positive_rate(y_pred1)\n",
    "print(y_pred1.shape)\n",
    "\n",
    "clf2 = lgb.Booster(\n",
    "    model_file='/content/drive/My Drive/DataChallenge2020MSBGD/model_save/final8.txt')\n",
    "y_pred2 = clf1.predict(xtest, num_iteration=clf2.best_iteration)\n",
    "\n",
    "clf3 = lgb.Booster(\n",
    "    model_file='/content/drive/My Drive/DataChallenge2020MSBGD/model_save/final7.txt')\n",
    "y_pred3 = clf1.predict(xtest, num_iteration=clf3.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual voting\n",
    "pred_avg = np.mean(y_pred1, y_pred2, y_pred3)\n",
    "print(y_pred1[1:5], y_pred2[1:5], y_pred3[1:5])\n",
    "print(pred_avg[1:5])\n",
    "\n",
    "np.savetxt('/content/drive/My Drive/DataChallenge2020MSBGD/ytest_challenge_student27A.csv',\n",
    "           pred_avg, fmt='%1.15f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T10:59:28.145532Z",
     "start_time": "2020-02-20T10:58:55.624157Z"
    }
   },
   "outputs": [],
   "source": [
    "# explore the different results\n",
    "# only for test\n",
    "y1 = np.loadtxt('ytest_challenge_student_WRF5100_alldata.csv',\n",
    "                delimiter=',', skiprows=1)\n",
    "y2 = np.loadtxt('ytest_challenge_student7.csv', delimiter=',', skiprows=1)\n",
    "print(y1[1:5])\n",
    "print(y2[1:5])\n",
    "\n",
    "y1 = y1[np.argsort(y1)]\n",
    "y2 = y2[np.argsort(y1)]\n",
    "\n",
    "pred1 = np.where(y1 >= 0.5, 1, 0)\n",
    "pred2 = np.where(y2 >= 0.5, 1, 0)\n",
    "\n",
    "diff = np.where(pred1 != pred2, y1-y2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T12:26:56.373152Z",
     "start_time": "2020-02-20T12:26:56.117270Z"
    }
   },
   "outputs": [],
   "source": [
    "# diff y1 y2\n",
    "y3 = (y1+y2)/2\n",
    "pred3 = np.where(y3 >= 0.5, 1, 0)\n",
    "\n",
    "diff10_12 = np.where(pred1 > pred2, pred1-pred2, 0)\n",
    "diff01_12 = np.where(pred1 < pred2, pred1-pred2, 0)\n",
    "diff10_13 = np.where(pred1 > pred3, pred1-pred3, 0)\n",
    "diff01_13 = np.where(pred1 < pred3, pred1-pred3, 0)\n",
    "diff10_23 = np.where(pred2 > pred3, pred2-pred3, 0)\n",
    "diff01_23 = np.where(pred2 < pred3, pred2-pred3, 0)\n",
    "print(diff10_12.sum(),\n",
    "      diff01_12.sum(),\n",
    "      diff10_13.sum(),\n",
    "      diff01_13.sum(),\n",
    "      diff10_23.sum(),\n",
    "      diff01_23.sum())\n",
    "\n",
    "print(diff01.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T11:04:57.149705Z",
     "start_time": "2020-02-20T11:04:15.463355Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.scatter(range(len(diff2)), diff2, alpha=0.1, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T11:00:10.427147Z",
     "start_time": "2020-02-20T10:59:28.187292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.scatter(range(len(diff)), diff, alpha=0.1, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T11:06:54.784924Z",
     "start_time": "2020-02-20T11:06:53.887449Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.hist(diff2,alpha=0.4, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_train=np.vstack((y_pred1,y_pred2,y_pred3)).transpose()\n",
    "stacking_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:12:19.854229Z",
     "start_time": "2020-02-25T21:51:05.268Z"
    }
   },
   "outputs": [],
   "source": [
    "#1st stacking algo \n",
    "stacking= LogisticRegression(solver='lbfgs',class_weight={0: 5800, 1: 1})\n",
    "stacking.fit(stacking_train[:1000000,:], y_train[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T21:28:37.347988Z",
     "start_time": "2020-02-25T20:37:57.704Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2nd stacking algo\n",
    "stacking = RandomForestClassifier(class_weight={0: 5800, 1: 1}, max_features='sqrt',\n",
    "                                  bootstrap=True,  n_estimators=150,\n",
    "                                  n_jobs=-1)  # ,  n_estimators=1000)\n",
    "stacking.fit(stacking_train[:1000000, :], y_train[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:12:19.858206Z",
     "start_time": "2020-02-25T21:51:15.903Z"
    }
   },
   "outputs": [],
   "source": [
    "#train the stacking model\n",
    "y_test_stacking1 = clf1.predict(x_test, num_iteration=clf1.best_iteration)\n",
    "\n",
    "y_test_stacking2 = clf1.predict(x_test, num_iteration=clf2.best_iteration)\n",
    "\n",
    "y_test_stacking3 = clf1.predict(x_test, num_iteration=clf3.best_iteration)\n",
    "\n",
    "stacking_test=np.vstack((y_test_stacking1,y_test_stacking2,y_test_stacking3)).transpose()\n",
    "print(stacking_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:12:19.861024Z",
     "start_time": "2020-02-25T21:51:19.073Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare the predction of stacking and single classifier\n",
    "predict1 = clf1.predict(x_test)\n",
    "predict2 = clf2.predict(x_test)\n",
    "predict3 = clf3.predict(x_test)\n",
    "\n",
    "stacking_preds_test = stacking.predict_proba(stacking_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:12:19.865380Z",
     "start_time": "2020-02-25T21:51:26.134Z"
    }
   },
   "outputs": [],
   "source": [
    "stacking_preds_test=stacking.predict_proba(stacking_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RF\n",
    "# Load test data\n",
    "xtest = np.loadtxt(\n",
    "    '/content/drive/My Drive/DataChallenge2020MSBGD/xtest_challenge.csv', delimiter=',', skiprows=1)\n",
    "# Classify the provided test data\n",
    "ytest = clf.predict_proba(xtest)[:, clf.classes_ == 1][:, 0]\n",
    "print(ytest.shape)\n",
    "np.savetxt('/content/drive/My Drive/DataChallenge2020MSBGD/ytest_challenge_student7.csv',\n",
    "           ytest, fmt='%1.15f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:52:32.325749Z",
     "start_time": "2020-02-19T21:46:00.454449Z"
    }
   },
   "outputs": [],
   "source": [
    "# for GBM\n",
    "\n",
    "bst = lgb.Booster(\n",
    "    model_file='/content/drive/My Drive/DataChallenge2020MSBGD//model_save/final5.txt')\n",
    "# Load test data\n",
    "xtest = np.loadtxt(\n",
    "    '/content/drive/My Drive/DataChallenge2020MSBGD/xtest_challenge.csv', delimiter=',', skiprows=1)\n",
    "# Classify the provided test data\n",
    "ytest = bst.predict(xtest)  # [:,0]\n",
    "print(ytest.shape)\n",
    "print(ytest[1:10])\n",
    "np.savetxt('/content/drive/My Drive/DataChallenge2020MSBGD/ytest_challenge_studentfinal5.csv',\n",
    "           ytest, fmt='%1.15f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T09:51:11.045522Z",
     "start_time": "2020-02-19T09:51:10.956349Z"
    }
   },
   "source": [
    "## Submit history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- 2020.2.17 1st summit : 0.8501 \n",
    "    - ranked 16th/31\n",
    "    *     Weighted Random Forest with class_weight {5100:1} \n",
    "\n",
    "\n",
    "- 2020.2.17 2nd summit : 0.8458 \n",
    "    - ranked 16th/31  \n",
    "    -    ( baseline Lightgbm)\n",
    "\n",
    "\n",
    "- 2020.2.18  summit3 : 0.07 (format error)\n",
    "\n",
    "\n",
    "- 2020.2.19 summit4 : 0.8730 \n",
    "    - ranked 9th/34\n",
    "    *  Lightgbm with L1 L2 regularisation, decay learning rate \n",
    "    \n",
    "\n",
    "- 2020.2.19 summit5 : 0.875682828873 \n",
    "    - ranked 5th/43\n",
    "    *  Lightgbm with custom loss function(beta=5), validation=0.9880 \n",
    "    \n",
    "\n",
    "- 2020.2.27 summit6: 0.887100497022 \n",
    "    - ranked 1st/50\n",
    "    * Lgbm with data augementation , (beta = 2, 1000leaves, max depth=10),validation=0.997287 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "name": "DataChallengeYang_WANG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
